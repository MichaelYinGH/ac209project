{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from os import path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "- We decided to remove rows with null values (given the high abundance of training examples) and to remove rows which included a stray value one day in New York as per https://github.com/CSSEGISandData/COVID-19/issues/3103.\n",
    "- We assigned the 14-day covid history as separate predictors, as well as a few demographic indicators\n",
    "- We standardized the data given high variance between the ranges of different features\n",
    "- We used one-hot encoding for categorical predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_date(i):\n",
    "    if i>9:\n",
    "        return str(i)\n",
    "    return \"0\" + str(i)\n",
    "\n",
    "augmented_df = pd.read_csv(\"../processed_data/combined.csv\", index_col=0)\n",
    "\n",
    "# expand index to multi-index\n",
    "augmented_df = augmented_df.set_index([\"state\", \"fips\"], append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "augmented_df = augmented_df.dropna(subset=([f\"{k}_before\" for k in range(14)] + ['all_poverty', 'pct_impoverished']))\n",
    "\n",
    "# remove data with spike from NY county https://github.com/CSSEGISandData/COVID-19/issues/3103\n",
    "augmented_df = augmented_df.drop(augmented_df.loc[[(\"08-31\", \"New York\", 36061)]+[(f\"09-{num_to_date(i)}\", \"New York\", 36061) for i in range(1,15)]].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['state_code', 'c4', 'c6', 'protest_size']\n",
    "for pred in cat:\n",
    "    one_hot_encoding = pd.get_dummies(augmented_df[pred], prefix=pred)\n",
    "    augmented_df = augmented_df.join(one_hot_encoding).drop(columns=[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "We did not employ a random train-test split because of the correlations between adjacent examples in the dataset given nature of time-series data. Thust, we split our data into train-test sets at a given date, and then shuffled each set to ensure a more evenly distributed training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = augmented_df.drop(columns=['0_before', 'male', 'female', 'county'])\n",
    "y = augmented_df['0_before']\n",
    "\n",
    "test_size = 0.1\n",
    "split_idx = int(X.shape[0] * (1-test_size))\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=209)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=209)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale non-categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_categorical = ['median_age', 'population',\n",
    "       'female_percentage', 'lat', 'long', 'life_expectancy', 'mortality_risk',\n",
    "       'all_poverty', 'sq_miles', 'pct_none', 'pct_hs', 'pct_bachelors',\n",
    "       'median_household_income', 'pct_black', 'pct_asian', 'pct_hispanic',\n",
    "       'pct_non_hispanic_white', 'pct_not_proficient_in_english', 'pct_rural',\n",
    "       'pct_impoverished', 'pop_density', 'r_voteshare',\n",
    "       'stringency', '13_before', '12_before', '11_before',\n",
    "       '10_before', '9_before', '8_before', '7_before', '6_before', '5_before',\n",
    "       '4_before', '3_before', '2_before', '1_before']\n",
    "\n",
    "scaler = StandardScaler().fit(X_train[non_categorical])\n",
    "\n",
    "# only scale non-categorical variables\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled_noncat = scaler.transform(X_train[non_categorical])\n",
    "for (i, predictor) in enumerate(non_categorical):\n",
    "    X_train_scaled[predictor] = X_train_scaled_noncat[:, i]\n",
    "\n",
    "# only scale non-categorical variables\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled_noncat = scaler.transform(X_test[non_categorical])\n",
    "for (i, predictor) in enumerate(non_categorical):\n",
    "    X_test_scaled[predictor] = X_test_scaled_noncat[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348947, 1, 20)\n",
      "(348947, 74)\n"
     ]
    }
   ],
   "source": [
    "# Divide each example into static and time-series data\n",
    "\n",
    "# reshape sequences for RNN cells\n",
    "seq_cols = [f\"{k}_before\" for k in range(1, 14)] + [f\"protest_size_{k}.0\" for k in range(-1, 6)]\n",
    "X_train_seq = X_train_scaled[seq_cols].values.reshape(-1, 1, 20)\n",
    "print(X_train_seq.shape)\n",
    "\n",
    "# slice off demographic data\n",
    "X_train_demo = X_train_scaled.drop(columns=seq_cols)\n",
    "print(X_train_demo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer for sequence- shape is batch_size/1/window_size\n",
    "seq_in = keras.Input(shape=(1, X_train_seq.shape[2],))\n",
    "\n",
    "# input layer for demographic data\n",
    "demo_in = keras.Input(shape=(X_train_demo.shape[1],))\n",
    "\n",
    "# network for sequences\n",
    "h1_seq = l.LSTM(32)(seq_in)\n",
    "\n",
    "# network for demographics with regularization\n",
    "h1_demo = l.Dense(32, activation='relu')(demo_in)\n",
    "h1_demo = l.Dropout(0.1)(h1_demo)\n",
    "\n",
    "# concat\n",
    "x = l.concatenate([h1_seq, h1_demo])\n",
    "\n",
    "# dense on top of concatenated layer\n",
    "x = l.Dense(16, activation='relu')(x)\n",
    "x = l.Dropout(0.1)(x)\n",
    "\n",
    "out = l.Dense(1)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and summarize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 74)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 1, 20)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           2400        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 32)           6784        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           lstm[0][0]                       \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1040        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            17          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,241\n",
      "Trainable params: 10,241\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[seq_in, demo_in],\n",
    "    outputs=[out],\n",
    ")\n",
    "\n",
    "# use early stopping and save best model to reload it\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir='../logs/fit/'+datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint('optimum.h5', monitor=\"val_loss\", save_best_only=True)\n",
    "\n",
    "model.compile(loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cc22f23b73a0bf8b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cc22f23b73a0bf8b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>\n",
      "Train on 279157 samples, validate on 69790 samples\n",
      "Epoch 1/50\n",
      "279157/279157 [==============================] - 80s 285us/sample - loss: 4649.4405 - val_loss: 2467.7191\n",
      "Epoch 2/50\n",
      "279157/279157 [==============================] - 60s 214us/sample - loss: 4055.1133 - val_loss: 2210.6255\n",
      "Epoch 3/50\n",
      "279157/279157 [==============================] - 62s 221us/sample - loss: 3761.2363 - val_loss: 2039.9898\n",
      "Epoch 4/50\n",
      "279157/279157 [==============================] - 61s 220us/sample - loss: 3540.0115 - val_loss: 1830.2597\n",
      "Epoch 5/50\n",
      "279157/279157 [==============================] - 63s 226us/sample - loss: 3421.1701 - val_loss: 1822.8075\n",
      "Epoch 6/50\n",
      "279157/279157 [==============================] - 78s 280us/sample - loss: 3315.4603 - val_loss: 1684.2710\n",
      "Epoch 7/50\n",
      "279157/279157 [==============================] - 67s 240us/sample - loss: 3321.9550 - val_loss: 1659.6846\n",
      "Epoch 8/50\n",
      "279157/279157 [==============================] - 81s 291us/sample - loss: 3234.3143 - val_loss: 1659.9584\n",
      "Epoch 9/50\n",
      "279157/279157 [==============================] - 78s 279us/sample - loss: 3248.7182 - val_loss: 1621.8781\n",
      "Epoch 10/50\n",
      "279157/279157 [==============================] - 81s 289us/sample - loss: 3218.2916 - val_loss: 1631.2507\n",
      "Epoch 11/50\n",
      "279157/279157 [==============================] - 79s 284us/sample - loss: 3179.0899 - val_loss: 1663.7452\n",
      "Epoch 12/50\n",
      "279157/279157 [==============================] - 74s 264us/sample - loss: 3139.6864 - val_loss: 1651.5648\n",
      "Epoch 13/50\n",
      "279157/279157 [==============================] - 81s 291us/sample - loss: 3195.0187 - val_loss: 1701.5615\n",
      "Epoch 14/50\n",
      "279157/279157 [==============================] - 87s 313us/sample - loss: 3130.6817 - val_loss: 1617.6068\n",
      "Epoch 15/50\n",
      "279157/279157 [==============================] - 79s 284us/sample - loss: 3151.9057 - val_loss: 1659.6115\n",
      "Epoch 16/50\n",
      "279157/279157 [==============================] - 66s 238us/sample - loss: 3214.9958 - val_loss: 1672.0351\n",
      "Epoch 17/50\n",
      "279157/279157 [==============================] - 100s 356us/sample - loss: 3100.4946 - val_loss: 1615.3636\n",
      "Epoch 18/50\n",
      "279157/279157 [==============================] - 80s 287us/sample - loss: 3048.4524 - val_loss: 1580.0330\n",
      "Epoch 19/50\n",
      "279157/279157 [==============================] - 80s 286us/sample - loss: 3060.2076 - val_loss: 1576.9863\n",
      "Epoch 20/50\n",
      "279157/279157 [==============================] - 81s 290us/sample - loss: 2993.5195 - val_loss: 1514.2281\n",
      "Epoch 21/50\n",
      "279157/279157 [==============================] - 96s 345us/sample - loss: 2972.4598 - val_loss: 1506.8741\n",
      "Epoch 22/50\n",
      "279157/279157 [==============================] - 96s 344us/sample - loss: 2956.1946 - val_loss: 1499.1506\n",
      "Epoch 23/50\n",
      "279157/279157 [==============================] - 86s 309us/sample - loss: 2995.5047 - val_loss: 1479.8225\n",
      "Epoch 24/50\n",
      "279157/279157 [==============================] - 76s 274us/sample - loss: 2966.0738 - val_loss: 1533.0133\n",
      "Epoch 25/50\n",
      "279157/279157 [==============================] - 77s 277us/sample - loss: 2885.9658 - val_loss: 1545.1814\n",
      "Epoch 26/50\n",
      "279157/279157 [==============================] - 82s 292us/sample - loss: 2850.0860 - val_loss: 1476.0137\n",
      "Epoch 27/50\n",
      "279157/279157 [==============================] - 83s 296us/sample - loss: 2907.3947 - val_loss: 1439.9952\n",
      "Epoch 28/50\n",
      "279157/279157 [==============================] - 80s 288us/sample - loss: 2972.5743 - val_loss: 1416.4982\n",
      "Epoch 29/50\n",
      "279157/279157 [==============================] - 101s 362us/sample - loss: 2954.4222 - val_loss: 1414.6408\n",
      "Epoch 30/50\n",
      "279157/279157 [==============================] - 81s 291us/sample - loss: 2942.5513 - val_loss: 1481.8423\n",
      "Epoch 31/50\n",
      "279157/279157 [==============================] - 90s 323us/sample - loss: 2900.4862 - val_loss: 1514.3846\n",
      "Epoch 32/50\n",
      "279157/279157 [==============================] - 83s 296us/sample - loss: 2832.8570 - val_loss: 1450.0787\n",
      "Epoch 33/50\n",
      "279157/279157 [==============================] - 84s 300us/sample - loss: 2810.4231 - val_loss: 1603.9560\n",
      "Epoch 34/50\n",
      "279157/279157 [==============================] - 89s 320us/sample - loss: 2885.5656 - val_loss: 1467.0487\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir ../logs/fit\n",
    "\n",
    "model.fit(x=[X_train_seq, X_train_demo], y=y_train, epochs=50, validation_split=0.2, callbacks=[tensorboard_callback, early_stopping_callback, checkpoint_callback])\n",
    "\n",
    "model = keras.models.load_model('optimum.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>\n",
      "348947/348947 [==============================] - 35s 100us/sample - loss: 2403.5419- loss:\n",
      "Train MSE: 2403.5418688863315\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>\n",
      "38772/38772 [==============================] - 4s 97us/sample - loss: 6571.3510\n",
      "Test MSE: 6571.350996094002\n"
     ]
    }
   ],
   "source": [
    "X_test_seq = X_test_scaled[seq_cols].values.reshape(-1, 1, 20)\n",
    "\n",
    "X_test_demo = X_test_scaled.drop(columns=seq_cols)\n",
    "\n",
    "print(f\"Train MSE: {model.evaluate(x=[X_train_seq, X_train_demo], y=y_train)}\")\n",
    "print(f\"Test MSE: {model.evaluate(x=[X_test_seq, X_test_demo], y=y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>\n",
      "10 Examples with largest MSEs from training set\n",
      "5-day case history:\n",
      "                      1_before  2_before  3_before  4_before  5_before  \\\n",
      "date  state    fips                                                      \n",
      "09-21 Texas    48201     553.0     819.0     817.0     857.0     435.0   \n",
      "04-18 New York 36061    4206.0    4844.0    7837.0    3702.0    3555.0   \n",
      "04-15 New York 36061    3702.0    3555.0    4900.0    5924.0    5356.0   \n",
      "07-16 Texas    48029       0.0     854.0     565.0    1046.0       0.0   \n",
      "08-16 Texas    48113     764.0     921.0     750.0     322.0     328.0   \n",
      "04-11 New York 36061    5356.0    5225.0    4927.0    4695.0    4630.0   \n",
      "04-09 New York 36061    4927.0    4695.0    4630.0    4245.0    6147.0   \n",
      "09-23 Texas    48029    3196.0     102.0    2581.0     173.0     162.0   \n",
      "04-10 New York 36061    5225.0    4927.0    4695.0    4630.0    4245.0   \n",
      "04-25 New York 36061    4618.0   -1442.0    3107.0    2955.0    2535.0   \n",
      "\n",
      "                      6_before  7_before  \n",
      "date  state    fips                       \n",
      "09-21 Texas    48201    2768.0    1396.0  \n",
      "04-18 New York 36061    4900.0    5924.0  \n",
      "04-15 New York 36061    5225.0    4927.0  \n",
      "07-16 Texas    48029     923.0     954.0  \n",
      "08-16 Texas    48113     621.0     973.0  \n",
      "04-11 New York 36061    4245.0    6147.0  \n",
      "04-09 New York 36061    5350.0    4370.0  \n",
      "09-23 Texas    48029     141.0     153.0  \n",
      "04-10 New York 36061    6147.0    5350.0  \n",
      "04-25 New York 36061    3128.0    8220.0  \n",
      "\n",
      "Actual case number:\n",
      "date   state     fips \n",
      "09-21  Texas     48201    14129.0\n",
      "04-18  New York  36061     8220.0\n",
      "04-15  New York  36061     7837.0\n",
      "07-16  Texas     48029     5980.0\n",
      "08-16  Texas     48113     5361.0\n",
      "04-11  New York  36061     5924.0\n",
      "04-09  New York  36061     5225.0\n",
      "09-23  Texas     48029    -2845.0\n",
      "04-10  New York  36061     5356.0\n",
      "04-25  New York  36061     4640.0\n",
      "Name: 0_before, dtype: float64\n",
      "\n",
      "Predicted case number\n",
      "[[1241.148  ]\n",
      " [1721.7279 ]\n",
      " [1940.4062 ]\n",
      " [1020.54724]\n",
      " [ 891.25885]\n",
      " [2143.3337 ]\n",
      " [1779.2986 ]\n",
      " [ 398.23633]\n",
      " [2193.0354 ]\n",
      " [1606.1407 ]]\n"
     ]
    }
   ],
   "source": [
    "train_preds = model.predict(x=[X_train_seq, X_train_demo])\n",
    "\n",
    "print(\"10 Examples with largest MSEs from training set\")\n",
    "mses = (train_preds[:, 0] - y_train) ** 2\n",
    "print(\"5-day case history:\")\n",
    "print(X_train.iloc[mses.argsort()[-10:][::-1]][[\"1_before\", \"2_before\", \"3_before\", \"4_before\", \"5_before\", \"6_before\", \"7_before\"]])\n",
    "print(\"\\nActual case number:\")\n",
    "print(y_train.iloc[mses.argsort()[-10:][::-1]])\n",
    "print(\"\\nPredicted case number\")\n",
    "print(train_preds[mses.argsort()[-10:][::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>\n",
      "10 Examples with largest MSEs from training set\n",
      "5-day case history:\n",
      "                          1_before  2_before  3_before  4_before  5_before  \\\n",
      "date  state        fips                                                      \n",
      "11-25 Rhode Island 44007       0.0       0.0       0.0       0.0       0.0   \n",
      "11-18 Rhode Island 44007       0.0       0.0       0.0       0.0       0.0   \n",
      "11-30 Texas        48439    1305.0       0.0       0.0       0.0    1302.0   \n",
      "11-27 Florida      12086       0.0    2120.0    1852.0    1499.0    1746.0   \n",
      "11-11 Rhode Island 44007       0.0       0.0       0.0       0.0       0.0   \n",
      "12-03 Texas        48113    1640.0    1179.0    1702.0    2303.0     982.0   \n",
      "11-15 Florida      12086    1187.0    1876.0    1205.0     718.0     394.0   \n",
      "11-28 Texas        48201     108.0     596.0    1859.0    2052.0    1060.0   \n",
      "11-29 Texas        48113     982.0       0.0       0.0    1368.0    1716.0   \n",
      "11-26 Florida      12086    2120.0    1852.0    1499.0    1746.0    1940.0   \n",
      "\n",
      "                          6_before  7_before  \n",
      "date  state        fips                       \n",
      "11-25 Rhode Island 44007       0.0    3704.0  \n",
      "11-18 Rhode Island 44007       0.0    2499.0  \n",
      "11-30 Texas        48439    1710.0    1124.0  \n",
      "11-27 Florida      12086    1940.0    2091.0  \n",
      "11-11 Rhode Island 44007       0.0    1989.0  \n",
      "12-03 Texas        48113       0.0       0.0  \n",
      "11-15 Florida      12086     523.0    1710.0  \n",
      "11-28 Texas        48201     831.0    1450.0  \n",
      "11-29 Texas        48113     541.0    1862.0  \n",
      "11-26 Florida      12086    2091.0    1945.0  \n",
      "\n",
      "Actual case number:\n",
      "date   state         fips \n",
      "11-25  Rhode Island  44007    4468.0\n",
      "11-18  Rhode Island  44007    3704.0\n",
      "11-30  Texas         48439    3356.0\n",
      "11-27  Florida       12086    3752.0\n",
      "11-11  Rhode Island  44007    2499.0\n",
      "12-03  Texas         48113    2122.0\n",
      "11-15  Florida       12086    2385.0\n",
      "11-28  Texas         48201      65.0\n",
      "11-29  Texas         48113    2303.0\n",
      "11-26  Florida       12086       0.0\n",
      "Name: 0_before, dtype: float64\n",
      "\n",
      "Predicted case number\n",
      "[[ 746.97186]\n",
      " [ 605.59973]\n",
      " [ 448.3806 ]\n",
      " [1454.6178 ]\n",
      " [ 714.91693]\n",
      " [ 462.48077]\n",
      " [ 836.5025 ]\n",
      " [1612.405  ]\n",
      " [ 816.27844]\n",
      " [1472.4816 ]]\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(x=[X_test_seq, X_test_demo])\n",
    "\n",
    "print(\"10 Examples with largest MSEs from training set\")\n",
    "mses = (test_preds[:, 0] - y_test) ** 2\n",
    "print(\"5-day case history:\")\n",
    "print(X_test.iloc[mses.argsort()[-10:][::-1]][[\"1_before\", \"2_before\", \"3_before\", \"4_before\", \"5_before\", \"6_before\", \"7_before\"]])\n",
    "print(\"\\nActual case number:\")\n",
    "print(y_test.iloc[mses.argsort()[-10:][::-1]])\n",
    "print(\"\\nPredicted case number\")\n",
    "print(test_preds[mses.argsort()[-10:][::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs109a)",
   "language": "python",
   "name": "cs109a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
