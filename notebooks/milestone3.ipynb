{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "import requests, censusdata, zipfile\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import urllib, json, requests\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. County-Level COVID-19 Case Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_dir = \"../raw_data/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/\"\n",
    "start_date_file = \"03-23-2020.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of confirmed cases by county and date\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for file in os.listdir(raw_dir):\n",
    "    if file[-4:] == \".csv\" and file >= start_date_file:\n",
    "        entry = pd.read_csv(raw_dir + file)\n",
    "        entry = entry.loc[(entry.Country_Region == \"US\") & (entry.FIPS.apply(str).str.len() == 7), [\"FIPS\", \"Confirmed\"]].astype(int)\n",
    "        entry = entry.set_index(\"FIPS\").rename(columns = {\"Confirmed\": file[:-4]})\n",
    "        entry = entry[~entry.index.duplicated()]\n",
    "        data_list.append(entry)\n",
    "        \n",
    "confirmed = pd.concat(data_list, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of new confirmed cases\n",
    "\n",
    "new_confirmed = confirmed.diff(axis = 1)\n",
    "new_confirmed[start_date_file[:-4]] = confirmed[start_date_file[:-4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confirmed.to_csv(\"../processed_data/new_confirmed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. US county demographic data EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Pull downloaded Kaggle data (requires auth) \n",
    "Basic county-level demographic data from JHU and US Census was downloaded from Kaggle to a CSV since download requires username / password authentication. The data contains the unique FIPS code, county / state names, male and female populations, median county age, as well as the latitude and longitude of the center of the county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_demos = pd.read_csv(raw_dir/'us_county.csv')\n",
    "kaggle_demos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Request IHME health data\n",
    "\n",
    "Some additional health data was downloaded from the Institute for Health Metrics and Evaluation (IHME). The data contains time series records of mortality risk and life expectancy measures across every county for several age ranges dating back to the 1980s. We cleaned and standardized this data to only include the most recent sample of county average life expectancy at birth and mortality risk for the 65 to 85 year old population (the most vulnerable to the effects of COVID-19). The data also includes a FIPS code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ihme_health.csv' not in [_.name for _ in raw_dir.glob('*')]:\n",
    "    h_url = 'http://ghdx.healthdata.org/sites/default/files/record-attached-files/'\n",
    "    h_file = 'IHME_USA_COUNTY_LE_MORTALITY_RISK_1980_2014_NATIONAL_STATES_DC_CSV.zip'\n",
    "    r = requests.get(h_url+h_file, stream=True)\n",
    "    with open(raw_dir/'ihme_health.zip', 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)\n",
    "    with zipfile.ZipFile(raw_dir/'ihme_health.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(raw_dir/'imhe_health/')\n",
    "    health_pth = Path(raw_dir/'imhe_health/')\n",
    "    assert health_pth.exists(), 'no health files found'\n",
    "    files = health_pth.glob('*.csv')\n",
    "    \n",
    "    health_dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            max_year = df.year_id.max()\n",
    "            # logic to get single rows for pivot table\n",
    "            df = df[(df.year_id == max_year) & (df.sex == \"Both\") \\\n",
    "                    & ((df.age_name == \"0\") | (df.age_name == \"65 to 85\"))]\n",
    "            pvt = pd.pivot_table(df, values = 'val', columns = 'measure_name',\n",
    "                    index=['FIPS', 'location_name']).reset_index()\n",
    "            health_dfs.append(pvt)\n",
    "        except Exception as e:\n",
    "            print(e); continue\n",
    "            \n",
    "    health = pd.concat([_ for _ in health_dfs if _.shape[0] > 1])\n",
    "    health.to_csv(raw_dir/'ihme_health.csv')\n",
    "else:\n",
    "    health = pd.read_csv(raw_dir/'ihme_health.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Census income data\n",
    "\n",
    "We downloaded county-level income data from the most recent Census survey (2018) and processed the data to find the estimated number of impoverished people in each county, as well as the median household income. The data again includes a FIPS code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www2.census.gov/programs-surveys/saipe/datasets/2016/2016-state-and-county/est16all.txt')\n",
    "t = r.text\n",
    "\n",
    "lines = t.split('\\n')\n",
    "income_dict = {}\n",
    "for l in lines:\n",
    "    name = l[193:238].strip()\n",
    "    if name == '': continue\n",
    "    fin_demos = {\n",
    "        'fips_state': l[0:2].strip(),\n",
    "        'fips_county': l[3:6].strip(),\n",
    "        'all_poverty': l[7:15].strip(),\n",
    "        'median_hh_income': l[133:139].strip()\n",
    "    }\n",
    "    income_dict[name] = fin_demos\n",
    "\n",
    "income = pd.DataFrame(income_dict).T\n",
    "def fix_fips(s):\n",
    "    if len(s) == 3: return s\n",
    "    elif len(s) == 2: return f'0{s}'\n",
    "    else: return f'00{s}'\n",
    "    \n",
    "# fix erros in Kalawao county\n",
    "income = income.replace('.', np.nan)\n",
    "income['all_poverty'] = income.all_poverty.astype(float)\n",
    "income['median_hh_income'] = income.median_hh_income.astype(float)\n",
    "\n",
    "income['fips_county_pad'] = income.fips_county.map(fix_fips)\n",
    "income['fips'] = (income.fips_state + income.fips_county_pad).astype(int)\n",
    "income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Land area\n",
    "\n",
    "Finally, we pull census data for the land area (in square miles) of each county as of 2010, which is the most recent year available. The data includes the FIPS code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landvar = 'LND110210D'\n",
    "land = pd.read_csv(raw_dir/'LND01.csv')[['STCOU', landvar]].rename(\n",
    "    columns={'STCOU':'fips', landvar:'sq_miles'})\n",
    "land.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Merge\n",
    "\n",
    "We merge all of these datasets on FIPS code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = pd.merge(\n",
    "    kaggle_demos, health.set_index('FIPS')[['Life expectancy', 'Mortality risk']],\n",
    "    left_on='fips', right_index=True).merge(\n",
    "        income.set_index('fips')[['all_poverty', 'median_hh_income']],\n",
    "        left_on='fips', right_index=True).merge(\n",
    "            land.set_index('fips'), left_on='fips', right_index=True)\n",
    "\n",
    "def fix_colname(s): return s.lower().strip().replace(' ', '_')\n",
    "demos.columns = [fix_colname(_) for _ in demos.columns]\n",
    "demos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Enrich\n",
    "\n",
    "We engineer two features that we intuitively believe may have some relationship to the spread of COVID-19. The first is related to the rate at which the population is impoverished. We have estimates from Census of the total number of inhabitants living below the poverty line, but here we standardize by the population of the county to arrive at an estimate of the percent of the county that is impoverished. We also use the population variable and the estimated land area variable to determine the population density in terms of inhabitants per square mile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos['pct_impoverished'] = demos.all_poverty.div(demos.population)\n",
    "demos['pop_density'] = demos.population.div(demos.sq_miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos.to_csv(processed_dir/'demographic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Data Understanding\n",
    "We'll visualize some key features to get a better understanding of the demographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 'median_age female_percentage life_expectancy mortality_risk \\\n",
    "median_hh_income pct_impoverished pop_density'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.clustermap(demos[features].corr(method='spearman'), annot=True, fmt='.1f', linewidths=0.5,\n",
    "              mask=np.eye(len(features)), cmap='RdYlGn', dendrogram_ratio=.1,\n",
    "              figsize=(6,6), cbar=False)\n",
    "cm.fig.subplots_adjust(top=.9)\n",
    "cm.cax.set_visible(False)\n",
    "cm.fig.suptitle('Hierarchical cluster of key demographic features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos.pct_impoverished.hist()\n",
    "plt.title('Histogram of population poverty rate by county')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos.plot.scatter('pct_impoverished', 'mortality_risk', alpha=0.3)\n",
    "plt.xlabel('Percent of county living in poverty')\n",
    "plt.ylabel('Estimated mortality risk')\n",
    "plt.title('Relationship between poverty and mortality risk')\n",
    "\n",
    "X = demos.dropna()\n",
    "lm = LinearRegression().fit(X.pct_impoverished.values.reshape(-1,1), X.mortality_risk)\n",
    "xs = np.linspace(0, 0.5)\n",
    "ys = lm.predict(xs.reshape(-1,1))\n",
    "r2 = lm.score(X.pct_impoverished.values.reshape(-1,1), X.mortality_risk)\n",
    "print(f\"R2 score of poverty and mortality risk is {r2:.3f}\")\n",
    "plt.plot(xs,ys, color='black')\n",
    "plt.legend([f'$R^2 = {r2:.2f}$', 'Samples'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demos.plot.scatter('pop_density', 'median_hh_income', alpha=0.3)\n",
    "plt.xscale('log'); plt.yscale('log')\n",
    "plt.xlabel('Log population density')\n",
    "plt.xlabel('Log median household income')\n",
    "plt.title('Relationship between poverty and mortality risk'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Protest Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protest_data = pd.read_csv(\"../raw_data/protest_AECLD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_search = r'no\\ report'\n",
    "empty = protest_data.NOTES.str.findall(empty_search)\n",
    "\n",
    "count_empty = 0\n",
    "for i in empty:\n",
    "    if i:\n",
    "        count_empty = count_empty + 1\n",
    "\n",
    "count_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = r'\\[size=(.*)\\]'\n",
    "nums = r'\\d+|few\\ dozen*|few\\ hundred*|few\\ thousand*|several\\ dozen*|several\\ hundred*|several\\ thousand*|dozen*|hundred*|thousand*'\n",
    "sizes = protest_data.NOTES.str.findall(sr)\n",
    "unique_sizes = [list(x) for x in set(tuple(x) for x in sizes)]\n",
    "size_temp_df = pd.DataFrame([''.join(x) for x in sizes], columns = ['string'])\n",
    "size_labs = size_temp_df.string.str.findall(nums)\n",
    "\n",
    "count_recovered = 0\n",
    "for i in size_labs:\n",
    "    if i:\n",
    "        count_recovered = count_recovered + 1\n",
    "\n",
    "count_recovered\n",
    "\n",
    "print(f'{count_recovered} of the {len(protest_data) - count_empty} non-emtpy records were retrieved.')\n",
    "\n",
    "size_cleaned = [None] * len(protest_data)\n",
    "\n",
    "dozen = 12\n",
    "few_dozen = 50\n",
    "hundred = 100\n",
    "few_hundred = 500\n",
    "thousand = 1000\n",
    "few_thousand = 5000\n",
    "\n",
    "for ii in range(len(size_labs)):\n",
    "    \n",
    "    vals = size_labs[ii]\n",
    "    \n",
    "    if any(st == \"few dozen\" for st in vals) | any(st == \"several dozen\" for st in vals):\n",
    "        size_cleaned[ii] = few_dozen\n",
    "    elif any(st == \"few hundred\" for st in vals) | any(st == \"several hundred\" for st in vals):\n",
    "        size_cleaned[ii] = few_hundred\n",
    "    elif any(st == \"few thousand\" for st in vals) | any(st == \"several thousand\" for st in vals):\n",
    "        size_cleaned[ii] = few_thousand\n",
    "    elif any(st == \"dozen\" for st in vals):\n",
    "        size_cleaned[ii] = dozen\n",
    "    elif any(st == \"hundred\" for st in vals):\n",
    "        size_cleaned[ii] = hundred\n",
    "    elif any(st == \"thousand\" for st in vals):\n",
    "        size_cleaned[ii] = thousand\n",
    "    else:\n",
    "        size_cleaned[ii] = np.mean(np.array(vals, dtype=np.float32).astype(np.float))\n",
    "\n",
    "print(len(size_cleaned))\n",
    "protest_data[\"size\"] = size_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protest_data[\"99 or less\"] = (protest_data[\"size\"] <= 99) * 1\n",
    "protest_data[\"100 to 499\"] = ((protest_data[\"size\"] > 99) &(protest_data[\"size\"] <= 499)) * 1\n",
    "protest_data[\"500 to 999\"] = ((protest_data[\"size\"] > 499) & (protest_data[\"size\"] <= 999)) * 1\n",
    "protest_data[\"1000 to 4999\"] = ((protest_data[\"size\"] > 999) & (protest_data[\"size\"] <= 4999)) * 1\n",
    "protest_data[\"more than 4999\"] = ((protest_data[\"size\"] > 4999)) * 1\n",
    "protest_data\n",
    "\n",
    "sizes = [\"99 or less\", \"100 to 499\", \"500 to 999\", \"1000 to 4999\", \"more than 4999\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert={'fips': lambda x: str(x), 'EVENT_DATE': lambda x: pd.to_datetime(x)}\n",
    "protest_fips = pd.read_csv(\"../processed_data/protests_fips.csv\", converters = convert)\n",
    "protest_fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protest_data.EVENT_DATE = pd.to_datetime(protest_data.EVENT_DATE)\n",
    "protest_data = protest_data.replace('', np.nan)\n",
    "protest_fips[\"EVENT_DATE\"] = pd.to_datetime(protest_fips.EVENT_DATE)\n",
    "protest_data_full = protest_data.join(protest_fips.fips,  how = \"left\")\n",
    "var_list = [\"fips\", \"EVENT_DATE\",\"99 or less\", \"100 to 499\", \"500 to 999\", \"1000 to 4999\", \"more than 4999\"]\n",
    "protest_data_full\n",
    "protest_data_clean = protest_data_full[var_list]\n",
    "protest_data_clean[\"unknown\"] = (protest_data_clean[[\"99 or less\", \"100 to 499\",\"500 to 999\", \"1000 to 4999\", \"more than 4999\"]].sum(axis = 1) == 0).astype(int)\n",
    "protest_data_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protest_data_clean.to_csv(\"../processed_data/protests.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 FIPS Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_codes = []\n",
    "\n",
    "for i in range(len(protest_data)):\n",
    "    \n",
    "    temp_url = \"https://geo.fcc.gov/api/census/area?lat=\" +str(protest_data.LATITUDE[i]) +\"&lon=\" + str(protest_data.LONGITUDE[i])+ \"&format=json\"\n",
    "    \n",
    "    with urllib.request.urlopen(temp_url) as url:\n",
    "        data = json.loads(url.read().decode())\n",
    "    \n",
    "    if not data[\"results\"]:\n",
    "        fips_codes.append(\"\")\n",
    "    else:\n",
    "        fips_codes.append(data[\"results\"][0][\"county_fips\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protest_data[\"fips_county\"] = fips_codes\n",
    "protest_data[\"fips_county\"] = protest_data.fips_county.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Visualization and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert={'fips': lambda x: str(x), 'EVENT_DATE': lambda x: pd.to_datetime(x)}\n",
    "protest_df = pd.read_csv(\"../processed_data/protests.csv\", converters = convert)\n",
    "protest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data cleaning for plotting\n",
    "## exclude entires from before George Floyd's death\n",
    "protest_df = protest_df[protest_df.EVENT_DATE > \"2020-05-24\"]\n",
    "\n",
    "## collapse dummy variables to categories\n",
    "sizes = [\"99 or less\", \"100 to 499\", \"500 to 999\", \"1000 to 4999\", \"more than 4999\", \"unknown\"]\n",
    "protest_df[\"size\"] = protest_df[sizes].idxmax(axis = 1)\n",
    "protest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax = sns.countplot(x=\"size\", \n",
    "                   data=protest_df[protest_df[\"size\"] != \"unknown\"],\n",
    "                  order = [\"99 or less\", \"100 to 499\", \"500 to 999\", \"1000 to 4999\", \"more than 4999\"])\n",
    "ax.set_title(\"Counts of Protest Sizes\", fontsize = \"15\")\n",
    "ax.set_xlabel(\"Protest Size\")\n",
    "ax.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protest_df[\"size\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = pd.read_csv(\"../processed_data/demographic_data.csv\")\n",
    "demo_df.fips = [str(item).zfill(5) for item in demo_df.fips]\n",
    "protest_fips = protest_df.fips.unique()\n",
    "\n",
    "protest_fips_counts = protest_df.fips.value_counts().reset_index()\n",
    "protest_fips_counts.columns = [\"fips\", \"num\"]\n",
    "protest_fips_counts.sort_values(by = [\"num\"], inplace = True)\n",
    "\n",
    "print(f\"Of the total {len(demo_df)} FIPS code counties, {len(protest_fips)} had protests and {len(demo_df) - len(protest_fips)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (10,5))\n",
    "ax.hist(protest_fips_counts[\"num\"], 75)\n",
    "ax.set_xlabel(\"Number of Protests\")\n",
    "ax.set_ylabel(\"Number of Counties\")\n",
    "ax.set_title(\"Distribution of Numbers of Protests\", fontsize = \"15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = demo_df.merge(protest_fips_counts, on = [\"fips\"], how = \"left\")\n",
    "demo_df = demo_df.rename(columns = {\"num\":\"num_protests\"})\n",
    "demo_df[\"num_protests\"] = demo_df[\"num_protests\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [0, 1, 2]\n",
    "conditions = [\n",
    "    (demo_df[\"num_protests\"]==0),\n",
    "    (demo_df[\"num_protests\"] > 0) & (demo_df[\"num_protests\"] < 6),\n",
    "    (demo_df[\"num_protests\"] > 5)\n",
    "]\n",
    "\n",
    "demo_df[\"protests\"] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var = [\"median_age\", \"female_percentage\", \"life_expectancy\",\"pct_impoverished\"]\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize = (12,8))\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    demo_var = plot_var[i]\n",
    "    sns.kdeplot(data = demo_df[demo_df[\"protests\"] == 0][demo_var], ax = ax, label = \"No Protests\")\n",
    "    sns.kdeplot(data = demo_df[demo_df[\"protests\"] == 1][demo_var], ax = ax, label = \"5 or Fewer\")\n",
    "    sns.kdeplot(data = demo_df[demo_df[\"protests\"] == 2][demo_var], ax = ax, label = \"6 or More\")\n",
    "    ax.set_xlabel(demo_var)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(demo_var)\n",
    "fig.suptitle(\"Distributions of Demographic Data by Number of Protests\", y = 0.95, fontsize = \"15\")\n",
    "fig.subplots_adjust(hspace = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Creating the Prediction Dataset\n",
    "\n",
    "We aggregate the processed COVID-19 data with the demographic data, and create new rows in an augmented dataset including 14-day windows of COVID-19 cases from mid-March as distinct examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = pd.read_csv(\"../processed_data/new_confirmed.csv\")\n",
    "demographics_df = pd.read_csv(\"../processed_data/demographic_data.csv\")\n",
    "\n",
    "df = covid_df.merge(demographics_df, left_on=\"FIPS\", right_on=\"fips\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 15\n",
    "\n",
    "date_cols = df.columns[1:np.where(df.columns == 'fips')[0][0]]\n",
    "nondate_cols = df.columns[np.where(df.columns == 'fips')[0][0]:]\n",
    "\n",
    "augmented_data = []\n",
    "augmented_index = []\n",
    "    \n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    for i, col in enumerate(date_cols[:-WINDOW_SIZE]):\n",
    "        series = row[date_cols[i:i+WINDOW_SIZE]].reset_index(drop=True)\n",
    "        series_dict = {f\"{14 - k}_before\": v for k, v in series.to_dict().items()}\n",
    "        series_dict.update(row[nondate_cols].to_dict())\n",
    "        augmented_data.append(series_dict)\n",
    "        augmented_index.append(f\"{row['fips']}_{date_cols[i+WINDOW_SIZE-1]}\")\n",
    "\n",
    "augmented_df = pd.DataFrame(data=augmented_data, index=augmented_index)\n",
    "augmented_df = augmented_df[1:]\n",
    "augmented_df.to_csv(\"../processed_data/combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Preprocess Data\n",
    "\n",
    "- We decided to remove rows with null values (given the high abundance of training examples) and to remove rows which included a stray value one day in New York as per https://github.com/CSSEGISandData/COVID-19/issues/3103.\n",
    "- We assigned the 14-day covid history as separate predictors, as well as a few demographic indicators\n",
    "- We standardized the data given high variance between the ranges of different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_date(i):\n",
    "    if i>9:\n",
    "        return str(i)\n",
    "    return \"0\" + str(i)\n",
    "\n",
    "augmented_df = pd.read_csv(\"../processed_data/combined.csv\", index_col=0)\n",
    "\n",
    "# drop null values\n",
    "augmented_df = augmented_df.dropna(subset=([f\"{k}_before\" for k in range(15)]+['pct_impoverished']))\n",
    "\n",
    "# remove data with spike from NY county https://github.com/CSSEGISandData/COVID-19/issues/3103\n",
    "augmented_df = augmented_df.drop([\"36061_08-31-2020\"]+[f\"36061_09-{num_to_date(i)}-2020\" for i in range(1,15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [f\"{k + 1}_before\" for k in range(14)] + ['median_age', 'female_percentage', 'life_expectancy', 'pct_impoverished', 'median_hh_income']\n",
    "X = augmented_df[predictors]\n",
    "y = augmented_df['0_before']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Prediction\n",
    "\n",
    "- We fit a model to the scaled, cleaned data and report MSE and coefficient values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = LinearRegression()\n",
    "baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_predict = baseline.predict(X_train_scaled)\n",
    "y_test_predict = baseline.predict(X_test_scaled)\n",
    "\n",
    "train_mse = mean_squared_error(y_train_predict, y_train)\n",
    "test_mse = mean_squared_error(y_test_predict, y_test)\n",
    "\n",
    "print(f\"Train MSE: {train_mse}\\n Test MSE: {test_mse}\\n\")\n",
    "\n",
    "print(\"Coefficient values\")\n",
    "for coef, col in zip(baseline.coef_, predictors):\n",
    "    print(f\"{col}: {coef}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
